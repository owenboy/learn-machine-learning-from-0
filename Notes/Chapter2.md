# 第二章 模型评估与选择

## 2.1 经验误差与过拟合

错误率（error rate）：分类错误的样本数占样本总数的比例

精度（accuracy）：精度 = 1 - 错误率

误差（error）：学习器的预测输出与样本真实输出之间的差异

训练误差（training error）或经验误差（empirical error）：学习器在训练集上的误差

泛化误差（generalization error）：学习器在新样本上的误差

我们希望得到的是泛化误差小的学习器，也就是在新样本上表现好的学习器。

过拟合（overfitting）：把训练样本自身的特点当成普遍特点，导致泛化能力下降。（由于训练样本都是有限的，会存在专属于训练样本的特点，而这些特点对于训练样本之外的样本可能并不适用）

欠拟合（underfitting）：对训练样本的一般性质尚未学好

欠拟合比较容易克服，而过拟合就很麻烦，各类学习算法都必然带有一些针对过拟合的措施。过拟合是无法避免的，只能缓解。

模型选择（model selection）时希望泛化误差最小，但是我们无法直接获得泛化误差，现实中就需要对模型进行评估。

## 2.2 评估方法

测试集（testing set）：用来测试学习器对新样本的判别能力

测试误差（testing error）：在测试集上的误差，泛化误差的近似

> Note: 测试集应该尽量和训练集互斥

所以需要对给定的数据集进行适当的处理，产出训练集S和测试集T

#### 2.2.1 留出法
留出法（hold-out）直接将数据集划分为两个互斥的集合。另个集合交集为空，并集为整个集合。
> 但是需要注意，训练集和测试集的划分要尽可能保持数据分布的一致性。

保留类别比例的采样方式通常称为“分层采样”（stratified sampling）。

> 单次使用留出法得到的估计结果往往不够稳定可靠，在使用留出法时，一般要采用若干次随机划分、重复进行试验评估后取平均值作为留出法的评估结果。

#### 2.2.2 交叉验证法

#### 2.2.3 自助法

#### 2.2.4 调参与最终模型

## 2.3 性能度量

#### 2.3.1 错误率与精度

#### 2.3.2 查准率、查全率与F1

#### 2.3.3 ROC与AUC

#### 2.3.4 代价敏感错误率与代价曲线

## 2.4 比较检验

#### 2.4.1 假设检验

#### 2.4.2 交叉验证t检验

#### 2.4.3 McNemar检验

#### 2.4.4 Friedman检验与Nemenyi后续检验

## 2.5 偏差与方差

## 2.6 阅读材料

